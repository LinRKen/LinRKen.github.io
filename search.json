[{"title":"test","path":"/2024/04/23/test/","content":"醒目的引用"},{"title":"1.2 实数系基本定理","path":"/2024/04/23/1.2 实数系基本定理/","content":"有理数系与实数系共同点都具有：运算的封闭性、有序性、稠密性 不同点有理数系对极限运算不封闭实数系对极限运算封闭 [[01-本科课程/大二上/01-Calculus/纵向总结/单调有界定理]]定理单调有界数列必收敛. 证明见书$P_{12}$ 例题wjf/eg/1-2-2 (Euler-Mascheroni常数$\\gamma$)wjf/eg/1-2-3wjf/xt/1-10wjf/xt/1-12[[01-本科课程/大二上/01-Calculus/纵向总结/闭区间套定理]]定义(闭区间套)若闭区间列$\\{[a_n, b_n]\\}$满足 [a_{n+1}, b_{n+1}] \\subset [a_n, b_n] \\quad(n=1,2,...), \\quad \\lim_{n\\to\\infty}(b_n-a_n)=0则称$\\{[a_n, b_n]\\}$构成一个闭区间套 定理若$\\{[a_n, b_n]\\}$构成一个闭区间套, 则存在唯一的$\\xi\\in\\mathbb{R}$, 使得$\\xi\\in[a_n,b_n]\\quad(n=1,2,…)$, 且$\\lim_{n\\to\\infty}a_n=\\lim_{n\\to\\infty}b_n=\\xi$. 证明书$P_{15}$ 例题wjf/eg/1-2-4 (算数-几何平均值)wjf/eg/1-2-5wjf/xt/1-14wjf/xt/1-15[[01-本科课程/大二上/01-Calculus/纵向总结/Bolzano-Weierstrass定理]]定义(子列)设$\\{a_n\\}$为一个数列，而$n_1&lt;n_2&lt;…&lt;n_k&lt;n_{k+1}&lt;…$是一列严格单调增加的自然数，则$a_{n1},a_{n_2},…,a_{n_k},a_{n_{k+1}}$也是一个数列将其记为$\\{a_{n_k}\\}$, 且称$\\{a_{n_k}\\}$为数列$\\{a_n\\}$的一个子数列，简称子列. [[01-本科课程/大二上/01-Calculus/纵向总结/归并原理|归并原理]]$\\displaystyle\\lim_{n\\to\\infty}a_n=a$的充分必要条件是: 对于$\\{a_n\\}$的每个子列$\\{a_{n_k}\\}$都有$\\displaystyle\\lim_{n\\to\\infty}a_{n_k}=a$. 定理有界数列必有收敛子列. 证明书$P_{18}$ 例题wjf/eg/1-2-6wjf/xt/1-15wjf/xt/1-16[[01-本科课程/大二上/01-Calculus/纵向总结/Cauchy收敛原理]]定义(基本数列)若数列$a_n$满足: 对任意给定的$\\varepsilon&gt;0$都存在$N\\in\\mathbb{N}$, 使得对任意的$m,n&gt;N$都有$|a_m-a_n|&lt;\\varepsilon$, 则称$\\{a_n\\}$为基本数列(或Cauchy数列). 定理数列$\\{a_n\\}$收敛的充分必要条件是: $\\{a_n\\}$为基本数列. 证明书$P_{20}$ 例题wjf/eg/1-2-7wjf/eg/1-2-8 (压缩条件)wjf/xt/1-17[[01-本科课程/大二上/01-Calculus/纵向总结/确界存在定理]]定义(上/下确界)设$S\\in\\mathbb{R}$且$S eq\\emptyset$. 上确界若$\\beta$是$S$的最小上界，即对任意的$x\\in S$都有$x\\leq \\beta$且对任意的$\\varepsilon&gt;0$；都存在$x_\\varepsilon\\in S$使得$x_\\varepsilon&gt;\\beta-\\varepsilon$,则称$\\beta$为$S$的上确界，记为$\\beta=supS$. 下确界若$\\beta$是$S$的最大下界，即对任意的$x\\in S$都有$x\\geq \\alpha$且对任意的$\\varepsilon&gt;0$；都存在$x_\\varepsilon\\in S$使得$x_\\varepsilon&lt;\\alpha+\\varepsilon$,则称$\\alpha$为$S$的上确界，记为$\\beta=infS$. 定理非空有上界的实数集必有上确界；非空有下界的实数集必有下确界. 证明书$P_{23}$ 例题wjf/eg/1-2-9wjf/eg/1-2-10wjf/xt/1-18 (上/下确界唯一性)wjf/xt/1-19[[01-本科课程/大二上/01-Calculus/纵向总结/Heine-Borel有限覆盖定理]]开覆盖设$S\\subset\\mathbb{R}$且$S eq\\emptyset$. 若区间簇$\\displaystyle\\{\\mathcal{l}_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$满足$\\displaystyle\\bigcup_{\\alpha\\in\\mathcal{A}}l_\\alpha\\supset S$,则称$\\displaystyle\\{\\mathcal{l}_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$是$S$的一个开覆盖.特别地，当$\\displaystyle\\{\\mathcal{l}_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$中的所有区间$l_\\alpha$是开区间时，称$\\displaystyle\\{\\mathcal{l}_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$是$S$的一个开覆盖. 定理设$\\displaystyle\\{\\mathcal{l}_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$是闭区间$[a,b]$的一个开覆盖，则在$\\displaystyle\\{\\mathcal{l}_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$中必存在有限个开区间即可覆盖$[a,b]$, 即存在$l_{\\alpha_1},l_{\\alpha_2},…,l_{\\alpha_p}\\in\\{l_\\alpha\\}_{\\alpha\\in\\mathcal{A}}$使得$\\displaystyle\\bigcup_{k=1}^pl_{\\alpha_k}\\supset [a,b]$. 证明书$P_{25}$ 例题wjf/eg/1-2-11实数系基本定理的等价条性Heine-Borel有限覆盖定理$\\Rightarrow$单调有界定理证明书$P_{27}$"},{"title":"Hello World","path":"/2024/04/22/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"},{"title":"4种积分","path":"/wiki/Calculous/4种积分.html","content":"4种积分几种常见积分的关系可以用下图表示: 积分线Reimann积分(定积分)\\int_{a}^{b} f(x) \\, dx = \\int_{a}^{b} f(a+b-x) \\, dx. 二重积分\\iint_{D} f(x,y) \\, d\\sigma = \\int_{a}^{b} \\int_{c}^{d} f(x,y) \\, dx \\, dy. 三重积分\\iiint_{\\Omega} f(x,y,z) \\, dV = \\int_{a}^{b} \\int_{c}^{d} \\int_{e}^{f} f(x,y,z) \\, dx \\, dy \\, dz. 曲线积分线一型曲线积分\\int_{C} f(x,y) \\, ds = \\int_{a}^{b} f(x(t),y(t)) \\sqrt{x'(t)^{2} + y'(t)^{2}} \\, dt.一型曲线积分是对曲线弧长的积分. 二型曲线积分\\int_{C} P(x,y) \\, dx + Q(x,y) \\, dy = \\int_{a}^{b} P(x(t),y(t)) x'(t) \\, dt + Q(x(t),y(t)) y'(t) \\, dt.二型曲线积分是对坐标的积分. 曲面积分线第一型曲面积分\\iint_{\\Sigma} f(x,y,z) \\, dS = \\iint_{D} f(x(u,v),y(u,v),z(u,v)) \\lvert \\mathbf{r}_{u} \\times \\mathbf{r}_{v} \\rvert \\, du \\, dv.第一型曲面积分是对曲面面积的积分. 第二型曲面积分\\begin{align}& \\iint_{\\Sigma} P(x,y,z) \\, dy \\, dz + Q(x,y,z) \\, dz \\, dx + R(x,y,z) \\, dx \\, dy \\\\\\\\&= \\iint_{D} P(x(u,v),y(u,v),z(u,v)) \\lvert \\mathbf{r}_{u} \\times \\mathbf{r}_{v} \\rvert \\, dv \\, du. \\end{align}第二型曲面积分是对坐标的积分. 变换线Green公式\\int_{C} P(x,y) \\, dx + Q(x,y) \\, dy = \\iint_{D} \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) \\, dx \\, dy.Green公式的几何意义: 曲线积分等于曲线围成的区域的面积. Gauss公式\\begin{align}& \\iint_{\\Sigma} P(x,y,z) \\, dy \\, dz + Q(x,y,z) \\, dz \\, dx + R(x,y,z) \\, dx \\, dy \\\\\\\\&= \\iiint_{\\Omega} \\left( \\frac{\\partial P}{\\partial x} + \\frac{\\partial Q}{\\partial y} + \\frac{\\partial R}{\\partial z} \\right) \\, dx \\, dy \\, dz. \\end{align}Gauss公式的几何意义: 曲面积分等于曲面围成的区域的体积. Stokes公式\\begin{align}& \\int_{C} P \\, dx + Q \\, dy + R \\, dz \\\\\\\\&= \\iint_{S} \\left( \\frac{\\partial R}{\\partial y} - \\frac{\\partial Q}{\\partial z} \\right) \\, dy \\, dz + \\left( \\frac{\\partial P}{\\partial z} - \\frac{\\partial R}{\\partial x} \\right) \\, dz \\, dx + \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) \\, dx \\, dy.\\end{align}Stokes公式的几何意义: 曲线积分等于曲线围成的曲面的面积."},{"title":"基础知识","path":"/wiki/Matrix_Analysis/基础知识.html","content":"等价 &amp; 相似 &amp; 合同 等价 &amp; 相似 &amp; 合同等价: $A \\sim B$, 存在非奇异矩阵 $P$, $Q$, 使得 $A = P B Q^{-1}$.相似: $A \\sim B$, 存在非奇异矩阵 $P$, 使得 $A = P B P^{-1}$.合同: $A \\sim B$, 存在非奇异矩阵 $P$, 使得 $A = P^{T} B P$. 三种关系的强弱:等价 $\\Leftrightarrow$ $A,B$ 秩相同;相似 $\\Leftrightarrow$ 合同 $+$ 正负惯性指数相同;等价 $\\Leftrightarrow$ 合同 $+$ 特征值相同 $+$ 主对角线元素之和相同 $+$ 正负惯性系数相同 $+$ 矩阵的值相同三者的强弱关系如下图所示: 补充:酉相似$A \\sim B$, 存在酉矩阵 $U$, 使得 $A = U B U^{H}$. 实对称矩阵的特殊情况实对称矩阵相似必合同. 域 幺半群幺半群是一个集合 $S$ 和一个二元运算 $*$ 构成的代数结构 $(S, *)$, 满足以下条件:封闭性: 对于任意 $a, b \\in S$, 有 $a * b \\in S$;结合律: 对于任意 $a, b, c \\in S$, 有 $(a * b) * c = a * (b * c)$;单位元: 存在一个元素 $e \\in S$, 使得对于任意 $a \\in S$, 有 $a * e = e * a = a$. 群群是一个集合 $G$ 和一个二元运算 $*$ 构成的代数结构 $(G, *)$, 满足以下条件:$(G, *)$ 构成一个幺半群;逆元: 对于任意 $a \\in G$, 存在一个元素 $a^{-1} \\in G$, 使得 $a * a^{-1} = a^{-1} * a = e$. 特别地, 若群 $(G, *)$ 满足交换律, 则称其为阿贝尔群 环环是一个集合 $R$ 和两个二元运算 $+$ 和 $\\cdot$ 构成的代数结构 $(R, +, \\cdot)$, 满足以下条件:$(R, +)$ 构成一个阿贝尔群;$(R, \\cdot)$ 构成一个幺半群.$\\cdot$ 对 $+$ 的分配律成立. 域域是一个集合 $F$ 和两个二元运算 $+$ 和 $\\cdot$ 构成的代数结构 $(F, +, \\cdot)$, 满足以下条件:$(F, +, \\cdot)$ 构成一个环;$(F \\setminus \\{0\\}, \\cdot)$ 构成一个阿贝尔群. 常见域有: 实数域 $\\mathbb{R}$, 复数域 $\\mathbb{C}$, 有理数域 $\\mathbb{Q}$, 有限域 $\\mathbb{F}_{p}$."},{"title":"常见矩阵分解","path":"/wiki/Matrix_Analysis/常见矩阵分解.html","content":"三角分解 记 $R$, $L$ 分别为正线上三角矩阵和正线下三角矩阵; $\\tilde{R}$, $\\tilde{L}$ 分别为单位上三角矩阵和单位下三角矩阵; $U$, $V$ 为酉矩阵; $D$ 为对角矩阵.注: 以上矩阵具有逆运算, 乘积运算的结构不变性 (或酉不变性).$Q$ 代表正交矩阵.UR 分解 &amp; LU 分解若 $A \\in \\mathbb{C}^{m \\times n}$, 则 $A$ 可以唯一地分解为 $A = U_{1}R$, 或 $A = LU_{2}$.UR(LU) 分解可以被理解为对 $A$ 的列(行)空间进行 Gram-Schmidt 正交化, $U$ 为变化后得到的标准正交基, $R(L)$ 为 $A$ 在该基下的坐标. QR 分解 &amp; LQ 分解若 $A \\in \\mathbb{R}^{n \\times n}$, 则 $A$ 可以唯一地分解为 $A = Q_{1}R$, 或 $A = LQ_{2}$. QR(LQ) 分解是 UR(LU) 分解在实数方阵上的一个特例. 注: 以上分解都具有唯一性. 酉分解若 $A \\in \\mathbb{C}^{m \\times n}$, 则存在酉矩阵 $U$ 和 $V$ 使得 A = U\\begin{bmatrix} L & 0 \\\\\\\\ 0 & 0\\end{bmatrix}V. 谱分解 单纯矩阵代数重复度&amp;几何重复度代数重复度: 特征值 $\\lambda$ 在特征多项式 $f(\\lambda)$ 中的重数几何重复度: 特征值 $\\lambda$ 对应的特征子空间的维数 几何重复度 $\\leq$ 它的代数重复度! 单纯矩阵若对 $A$ 的每个特征值有, 代数重复度 $=$ 几何重复度, 则称 $A$ 是单纯矩阵. $A$ 是单纯矩阵 $\\Leftrightarrow$ $A$ 可对角化. 单纯矩阵的幂等分解设 $A$ 是单纯矩阵, 则 $A$ 可以分解为一系列幂等矩阵 $A_{i}\\ (i=1,2, \\cdots, n)$ 的加权和:A = \\sum_{i=1}^{n} \\lambda_{i} A_{i}其中, $\\lambda_{i}$ 是 $A$ 的特征值. 正规矩阵正规矩阵若 $A$ 满足 $AA^{H} = A^{H}A$, 则称 $A$ 是正规矩阵. 若 $A$ 为正规矩阵, 且 $A$ 与 $B$ 酉相似, 则 $B$ 为正规矩阵.$A$ 是正规矩阵 $\\Leftrightarrow$ $A$ 与对角矩阵 $D$ 酉相似. Schur 分解若 $A \\in \\mathbb{C}^{n \\times n}$, 则存在酉矩阵 $U$ 使得A = U R U^{H}其中 $R$ 是上三角矩阵, 且对角元为是 $A$ 的特征值. Hermitian 矩阵及其分解 Hermitian 矩阵Hermitian 矩阵若 $A \\in \\mathbb{C}^{n \\times n}$, 若 $A = A^{H}$, 则称 $A$ 是 Hermitian 矩阵, 若 $A = -A^{H}$, 则称 $A$ 是反-Hermitian 矩阵.注: 实对称矩阵是Hermitian矩阵的一个特例. Hermitian 矩阵 $A$ 性质: $A$ 的特征值是实数 (如量子力学中的力学量), 特征向量是正交的. $A$ 是正规矩阵 (酉相似于对角矩阵). (半)正定二次型若 $A \\in \\mathbb{C}^{n \\times n}$ 是 Hermitian 矩阵, 若对任意非零向量 $x \\in \\mathbb{C}^{n}$, 有f(x) = x^{H}Ax > 0 \\ (geq 0)则称 $f(x)$ 是正定 (半正定) 二次型, $A$ 是正定 (半正定) Hermitian 矩阵. 正定 Hermitian 矩阵 $A$ 性质: $A$ 的对角元素都是正数. $A$ 的特征值都是正数. $A$ 的顺序主子式都是正数. $A$ 与单位矩阵合同. 存在正定矩阵 $B$ 使得 $A = B^{k}$. 存在正线下三角矩阵 $L$ 使得 $A = LL^{H}$ (Cholesky 分解). $det(A) \\leq a_{11}a_{22} \\cdots a_{nn}$ (Hadamard 不等式). &lt;!— ## 最大秩分解最大秩分解最大秩分解若 $A \\in \\mathbb{C}^{m \\times n}_{r}$, 则存在矩阵 $B \\in \\mathbb{C}^{m \\times r}_{r}$ 和 $C \\in \\mathbb{C}^{r \\times n}_{r}$ 使得 $A = BC$. --> 奇异值分解 设 $A \\in \\mathbb{C}^{m \\times n}$, 则： $rank(A) = rank(A^{H}A) = rank(AA^{H})$. $A^{H}A, AA^{H}$ 的特征是均为非负实数 (为 Hermitian 矩阵). $A^{H}A, AA^{H}$ 的非零特征值是相同的. 正奇异值若 $A \\in \\mathbb{C}^{m \\times n}_{r}$, $A^{H}A$ 的特征值为:\\lambda_{1} \\geq \\lambda_{2} \\geq \\cdots \\geq \\lambda_{r} > 0, \\lambda_{r+1} = \\cdots = \\lambda_{n} = 0.则 $\\sigma_{i} = \\sqrt{\\lambda_{i}}$ 称为 $A$ 的正奇异值. 若 $A$ 与 $B$ 酉等价, 则 $A$ 和 $B$ 的奇异值相同. 奇异值分解设 $A \\in \\mathbb{C}^{m \\times n}_{r}$. $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} \\gt 0$ 是 $A$ 的 $r$ 个奇异值, 则存在酉矩阵 $U \\in \\mathbb{C}^{m \\times m}$ 和 $V \\in \\mathbb{C}^{n \\times n}$, 使得A = U\\begin{bmatrix} \\Sigma & 0 \\\\\\\\ 0 & 0\\end{bmatrix}V^{H}其中 $\\Sigma = diag(\\sigma_{1}, \\sigma_{2}, \\cdots, \\sigma_{r})$. 最大秩分解 最大秩分解设 $A \\in \\mathbb{C}^{m \\times n}_{r}$, 则存在矩阵 $B \\in \\mathbb{C}^{m \\times r}_{r}$ 和 $D \\in \\mathbb{C}^{r \\times n}_{r}$ 使得 A = BD. 例求以下矩阵的最大秩分解:A = \\begin{bmatrix} 1 & 3 & 2 & 1 & 4 \\\\ 2 & 6 & 1 & 0 & 7 \\\\ 3 & 9 & 3 & 1 & 11 \\\\\\end{bmatrix}.解:(行初等变换) 对 $A$ 进行初等行变换为简化行阶梯形矩阵 $\\tilde{A}$:A \\leftarrow\\begin{bmatrix} 1 & 3 & 0 & -\\frac{1}{3} & \\frac{10}{3} \\\\ 0 & 0 & 1 & \\frac{2}{3} & \\frac{1}{3} \\\\ 0 & 0 & 0 & 0 & 0 \\\\\\end{bmatrix} = \\tilde{A}.则 $A = BD$, 其中B =\\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\\\ 3 & 3 \\\\\\end{bmatrix},D =\\begin{bmatrix} 1 & 3 & 0 & -\\frac{1}{3} & \\frac{10}{3} \\\\ 0 & 0 & 1 & \\frac{2}{3} & \\frac{1}{3} \\\\\\end{bmatrix}(列初等变换) 对 $A$ 进行初等列变换为简化列阶梯形矩阵 $\\tilde{A}$:\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ \\end{bmatrix}= \\tilde{A}.则 $A = BD$, 其中B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\\\end{bmatrix}D = \\begin{bmatrix} 1 & 3 & 2 & 1 & 4 \\\\ 2 & 6 & 1 & 0 & 7 \\\\\\end{bmatrix}."},{"title":"常见空间","path":"/wiki/Matrix_Analysis/常见空间.html","content":"常见空间 线性空间设 $V$ 是一个非空集合, $F$ 是一个域, 若 $V$ 上定义了两个运算: 加法 $+$ 和数乘 $\\cdot$, 满足以下条件:$(V, +)$ 构成一个阿贝尔群;对任意 $\\alpha, \\beta \\in F$, $x, y \\in V$, 有结合律: $(\\alpha \\beta) x = \\alpha (\\beta x)$;分配律: $(\\alpha + \\beta) x = \\alpha x + \\beta x$;分配律: $\\alpha (x + y) = \\alpha x + \\alpha y$;恒等律: $1 x = x$. 则称 $V$ 是一个 $F$ 上的线性空间. 一些重要的线性空间: 赋范线性空间: 定义了范数 $\\lVert \\cdot \\rVert$ 的线性空间. 内积空间: 定义了内积 $\\langle \\cdot, \\cdot \\rangle$ 的线性空间. 欧式空间: 定义了内积的有限维实线性空间. 酉空间: 定义了内积的有限维复线性空间. Banach 空间: 完备的赋范线性空间. Hilbert 空间: 完备的内积空间. 再生核希尔伯特空间 再生核希尔伯特空间"},{"title":"常见范数","path":"/wiki/Matrix_Analysis/常见范数.html","content":"向量范数 向量范数映射 $\\lVert \\cdot \\rVert: \\mathbb{C}^{n} \\rightarrow \\mathbb{R}$, 若满足:正定性: $\\lVert x \\rVert \\geq 0$, 且 $\\lVert x \\rVert = 0 \\Leftrightarrow x = 0$;齐次性: $\\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert$;三角不等式: $\\lVert x + y \\rVert \\leq \\lVert x \\rVert + \\lVert y \\rVert$.则称 $\\lVert \\cdot \\rVert$ 是 $\\mathbb{C}^{n}$ 向量范数. 向量范数具有以下性质: $\\lVert 0 \\rVert = 0$; $x eq 0$ 时, $\\left\\lVert \\frac{1}{\\lVert x \\rVert} x \\right\\rVert = 1$; 对任意 $x \\in \\mathbb{C}^{n}$, $\\lVert -x \\rVert = \\lVert x \\rVert$; 对任意 $x, y \\in \\mathbb{C}^{n}$, $\\lvert \\lVert x-y \\rVert \\rvert \\leq \\lVert x-y \\rVert$. 常见向量范数 $p$ 范数: $\\lVert x \\rVert_{p} = \\left( \\sum_{i=1}^{n} |x_{i}|^{p} \\right)^{1/p}$, $p \\geq 1$; $1$ 范数: $\\lVert x \\rVert_{1} = \\sum_{i=1}^{n} |x_{i}|$; $2$ 范数: $\\lVert x \\rVert_{2} = \\sqrt{\\sum_{i=1}^{n} |x_{i}|^{2}}$; $\\infty$ 范数: $\\lVert x \\rVert_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$. 矩阵范数 矩阵范数设 $A = (a_{ij}) \\in P^{m \\times n}$, 映射 $\\lVert \\cdot \\rVert: P^{m \\times n} \\rightarrow \\mathbb{R}$, 若满足:正定性: $\\lVert A \\rVert \\geq 0$, 且 $\\lVert A \\rVert = 0 \\Leftrightarrow A = 0$;齐次性: $\\lVert \\alpha A \\rVert = |\\alpha| \\lVert A \\rVert$;三角不等式: $\\lVert A + B \\rVert \\leq \\lVert A \\rVert + \\lVert B \\rVert$.则称 $\\lVert \\cdot \\rVert$ 是 $P^{m \\times n}$ 向量范数. 常见矩阵范数 $1$ 范数: $\\lVert A \\rVert_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{m} |a_{ij}|$; $2$ 范数 (又称为 Frobenius 范数 $\\lVert A \\rVert_{F}$): $\\lVert A \\rVert_{2} = \\sqrt{\\rho(A^{*}A)}$; $\\infty$ 范数: $\\lVert A \\rVert_{\\infty} = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{n} |a_{ij}|$. 等价性设 $\\lVert \\cdot \\rVert_{1}$ 和 $\\lVert \\cdot \\rVert_{2}$ 是 $P^{m \\times n}$ 的两个范数, 则存在常数 $c_{1}, c_{2} &gt; 0$, 使得对任意 $A \\in P^{m \\times n}$, 有: c_{1} \\lVert A \\rVert_{1} \\leq \\lVert A \\rVert_{2} \\leq c_{2} \\lVert A \\rVert_{1}.常用矩阵范数等价性:设 $A \\in P^{m \\times n}$, 则有: $\\frac{1}{\\sqrt{n}} \\lVert A \\rVert_{1} \\leq \\lVert A \\rVert_{2} \\leq \\sqrt{n} \\lVert A \\rVert_{1}$ $\\frac{1}{\\sqrt{n}} \\lVert A \\rVert_{\\infty} \\leq \\lVert A \\rVert_{2} \\leq \\sqrt{n} \\lVert A \\rVert_{\\infty}$ $\\frac{1}{n} \\lVert A \\rVert_{\\infty} \\leq \\lVert A \\rVert_{1} \\leq n \\lVert A \\rVert_{\\infty}$ 酉不变性设 $A \\in \\mathbb{C}^{n \\times n}$, $U, V \\in \\mathbb{C}^{n \\times n}$ 是酉矩阵, 则 \\lVert A \\rVert_{F} = \\lVert UA \\rVert_{F} = \\lVert AV \\rVert_{F} = \\lVert UAV \\rVert_{F}.算子范数 相容范数设 $\\lVert \\cdot \\rVert_{a}$, $\\lVert \\cdot \\rVert_{b}$, $\\lVert \\cdot \\rVert_{c}$ 分别是 $P^{m \\times l}$, $P^{l \\times n}$, $P^{m \\times n}$ 的范数, 如果\\lVert AB \\rVert_{c} \\leq \\lVert A \\rVert_{a} \\lVert B \\rVert_{b},则称 $\\lVert \\cdot \\rVert_{a}$, $\\lVert \\cdot \\rVert_{b}$, $\\lVert \\cdot \\rVert_{c}$ 是相容的. 算子范数设 $\\lVert x \\rVert_{a}$ 是 $P^{n}$ 上的向量范数, 则\\lVert A \\rVert_{a} = \\max_{x eq 0} \\frac{\\lVert Ax \\rVert_{a}}{\\lVert x \\rVert_{a}} = \\max_{\\lVert x \\rVert_{u} = 1} \\lVert Au \\rVert_{a}是与向量范数 $\\lVert x \\rVert_{a}$ 相容的矩阵范数, 称为从属于 $\\lVert x \\rVert_{a}$ 的算子范数. 算子范数的性质 算子范数是所有与向量范数 $\\lVert x \\rVert_{a}$ 相容的矩阵范数中最小的. 算子范数是自相容矩阵范数. 常见算子范数 $1$ 范数 (极大列和): $\\lVert A \\rVert_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{m} |a_{ij}|$ $\\infty$ 范数 (极大行和): $\\lVert A \\rVert_{\\infty} = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{n} |a_{ij}|$ $2$ 范数 (谱范数): $\\lVert A \\rVert_{2} = \\sqrt{\\rho(A^{*}A)}$ 谱范数的性质 $\\lVert A \\rVert_{2} = \\lVert A^{H} \\rVert_{2} = \\lVert A^{T} \\rVert_{2} = \\lVert \\bar{A} \\rVert_{2}$ $\\lVert A^{H}A \\rVert_{2} = \\lVert AA^{H} \\rVert_{2} = \\lVert A \\rVert_{2}^{2}$ 酉不变性: $\\lVert A \\rVert_{2} = \\lVert UA \\rVert_{2} = \\lVert AV \\rVert_{2} = \\lVert UAV \\rVert_{2}$ $\\displaystyle \\lVert A \\rVert_{2} = \\max_{\\lVert x \\rVert_{2} = 1, \\lVert y \\rVert_{2} = 1} \\lvert y^{H}Ax \\rvert$ $\\lVert A \\rVert_{2}^{2} \\leq \\lVert A \\rVert_{2} \\cdot \\lVert A \\rVert_{\\infty}$ 条件数 条件数设 $A$ 是可逆矩阵m, 则称\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert为矩阵 $A$ 的条件数."},{"title":"广义逆矩阵","path":"/wiki/Matrix_Analysis/广义逆矩阵.html","content":"广义逆矩阵 广义逆矩阵设 $A \\in \\mathbb{C}^{m \\times n}$, 若矩阵 $G \\in \\mathbb{C}^{n \\times m}$ 满足:AGA = b, \\quad \\forall b in R(A),则称 $G$ 是 $A$ 的广义逆矩阵, 记为 $G = A^{-}$. $G$ 是 $A$ 的广义逆矩阵的充要条件是: AGA = A.推论:$rank(A^{-}) \\geq rank(A)$. \\begin{align} A{1} &= \\{ G \\vert G = A^{-} + U - A^{-}UAA^{-}, \\forall U \\in \\mathbb{C}^{m \\times n} \\} \\\\ &= \\{ G \\vert G = A^{-} + (E_{n} - A^{-}A)V + W(E_{m} - AA^{-}), \\forall V,W \\in \\mathbb{C}^{n \\times m} \\} \\end{align}性质： $(A^{T})^{-} = (A^{-})^{T}$, $(A^{H})^{-} = (A^{-})^{H}$; $A^{-}A$ 和 $AA^{-}$ 是投影(幂等)矩阵, 且 $rank(A) = rank(A^{-}A) = rank(AA^{-})$; $\\lambda ^{-1}A^{-}$ 是 $\\lambda A$ 的广义逆矩阵; 设 $B = SAT$, 其中 $S, T$ 是可逆矩阵, 则 $B^{-} = T^{-1}A^{-}S^{-1}$; $R(AA^{-}) = R(A), N(A^{-}A) = N(A)$. 推论:$A \\in \\mathbb{C}^{n \\times m}$, 则 $rank(A) = n \\Leftrightarrow A^{-}A = E_{n}$; $rank(A) = m \\Leftrightarrow AA^{-} = E_{m}$; 单边逆矩阵 单边逆矩阵设 $A \\in \\mathbb{C}^{m \\times n}$, 若矩阵 $G \\in \\mathbb{C}^{n \\times m}$ 满足:AGA = b, \\quad \\forall b in R(A),则称 $G$ 是 $A$ 的广义逆矩阵, 记为 $G = A^{-}$. $A$ 左可逆 $\\Leftrightarrow$ $A$ 列满秩 $\\Leftrightarrow$ $N(A) = \\{0\\}$.$A$ 右可逆 $\\Leftrightarrow$ $A$ 行满秩 $\\Leftrightarrow$ $R(A) = \\mathbb{C}^{m}$. 左可逆设 $A \\in \\mathbb{C}^{m \\times n}$, 若 $A$ 左可逆, 则 G = \\begin{pmatrix} A_{1}^{-1} - BA_{2}A_{1}^{-1} & B \\\\ \\end{pmatrix}P是 $A$ 的左逆矩阵, 其中 $B \\in \\mathbb{C}^{n \\times (m-n)}$ 的任意矩阵, 初等行变换矩阵 $P$ 满足 $PA = \\begin{pmatrix} A_{1} \\\\ A_{2} \\end{pmatrix}$, $A_{1}$ 是 $n$ 阶可逆矩阵.右可逆设 $A \\in \\mathbb{C}^{m \\times n}$, 若 $A$ 右可逆, 则 G = Q\\begin{pmatrix} A_{1}^{-1} - A_{1}^{-1}A_{2}D \\\\ D \\\\ \\end{pmatrix}是 $A$ 的右逆矩阵, 其中 $B \\in \\mathbb{C}^{(n-m) \\times m}$ 的任意矩阵, 初等行变换矩阵 $Q$ 满足 $AQ = \\begin{pmatrix} A_{1} &amp; A_{2} \\end{pmatrix}$, $A_{1}$ 是 $m$ 阶可逆矩阵. 左可逆设 $A \\in \\mathbb{C}^{m \\times n}$ 左可逆, 则方程组 $Ax = b$ 有解的充要条件是(E_{m} - AA_{L}^{-1})b = 0\\tag{1}\\label{eq:1}若 $\\eqref{eq:1}$ 成立, 则方程组 $Ax=b$ 有唯一解x = (A^{H}A)^{1}A^{H}b右可逆设 $A \\in \\mathbb{C}^{m \\times n}$ 右可逆, 则方程组 $Ax = b$ 对任意 $b$ 都有解, 若$b eq 0$, 则x = A_{R}^{-1}b是方程组的解. M-P 广义逆矩阵 $A^{+}$ M-P 广义逆矩阵 $A^{+}$设 $A \\in \\mathbb{C}^{m \\times n}$, 若矩阵 $G \\in \\mathbb{C}^{n \\times m}$ 满足:AGA = A, GAG = G, (GA)^{H} = GA, (AG)^{H} = AG,则称 $G$ 是 $A$ 的 M-P 广义逆矩阵, 记为 $G = A^{+}$. $A^{+}$ 求法设 $A \\in \\mathbb{C}^{m \\times n}$, 其最大秩分解为 $A = BD$, 则A^{+} = D^{H}(DD^{H})^{-1}(B^{H}B)^{-1}B^{H}是 $A$ 的 M-P 广义逆矩阵 $A^{+}$. A 的 M-P 广义逆矩阵 $A^{+}$ 是唯一的. 性质: $(A^{+})^{+} = A$; $(A^{T})^{+} = (A^{+})^{T}$, $(A^{H})^{+} = (A^{+})^{H}$; $A^{+} = (A^{H}A)^{+}A^{H} = A^{H}(AA^{H})^{+}$; $R(A^{+}) = R(A^{H})$ $AA^{+} = P_{R(A)}, A^{+}A = P_{R(A^{H})}$, 其中 $P_{x}$ 是 $x$ 的投影矩阵; $R(A) = R(A^{H})$ 的充要条件是 $AA^{+} = A^{+}A$. $(A^{H}A)^{+} = A^{+}(A^{H})^{+},\\ (AA^{H})^{+} = (A^{H})A^{+}$; $(A^{H}A)^{+} = A^{+}(AA^{H})^{+}A = A^{H}(AA^{H})^{+}(A^{H})^{+}$ $AA^{+} = (AA^{H})(AAA^{H})^{+} = (AA^{H})^{+}(AA^{H})$ $A^{+}A = (A^{H}A)(A^{H}A)^{+} = (A^{H}A)^{+}(A^{H}A)$ 设 $A \\in \\mathbb{C}^{m \\times l}, B \\in \\mathbb{C}^{l \\times n}$ (AB)^{+} = B^{+}A^{+} \\Leftrightarrow R(A^{H}AB) \\subset R(B)\\ 且\\ R(BB^{H}A^{H}) \\subset R(A^{H})自反广义逆矩阵 $A_{r}^{-}$ M-P 广义逆矩阵 $A^{+}$设 $A \\in \\mathbb{C}^{m \\times n}$, 若矩阵 $G \\in \\mathbb{C}^{n \\times m}$ 满足:AGA = A, GAG = G则称 $G$ 是 $A$ 的 M-P 广义逆矩阵, 记为 $G = A_{r}^{-}$. 任意矩阵 $A$ 的自反广义逆矩阵 $A_{r}^{-}$. $A_{r}^{-}$ 求法设 $X,Y \\in \\mathbb{C}^{n \\times m}$ 均为 $A$ 的广义逆矩阵, 则Z = XAY是 $A$ 的 自反广义逆矩阵 $A_{r}^{-}$. 设 $A^{-}$ 是 $A$ 的广义逆矩阵, 则 $A_{r}^{-} = A^{-}A$ 是 $A$ 的自反广义逆矩阵的充要条件是 rank(A) = rank(A^{-})以下等式知二推一: $rank(A) = rank(X)$ $AXA = A$ $XAX = X$ $X = (A^{H}A)^{-}A^{H}, Y=A^{H}(AA^{H})^{-}$ 都是 $A$ 的自反广义逆矩阵. $AA_{r}^{-}, A_{r}^{-}A$ 都是投影(幂等)矩阵."},{"title":"常用级数","path":"/wiki/Calculous/常用级数.html","content":"数项级数 &amp; 函数项级数 级数的收敛数项级数 &amp; 函数项级数数项级数: $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$, $a_{n} \\in \\mathbb{R}$;函数项级数: $\\displaystyle \\sum_{n=1}^{\\infty} f_{n}(x)$, $f_{n}(x)$ 是定义在 $\\mathcal{I}$ 上的函数序列. 部分和 &amp; 部分和函数列数项级数的部分和: $\\displaystyle S_{n} = \\sum_{k=1}^{n} a_{k}$;函数项级数的部分和函数列: $\\displaystyle S_{n}(x) = \\sum_{k=1}^{n} f_{k}(x)$. 数项级数收敛若数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 的部分和序列 $\\{S_{n}\\}$ 的极限 $\\displaystyle \\lim_{n \\to \\infty} S_{n} = S$ 存在, 则称级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 收敛, 记为 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n} = S$.函数项级数逐点收敛若函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} f_{n}(x)$ 对任意给定的 $x \\in \\mathcal{I}$, 数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} f_{n}(x)$ 收敛, 则称函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} f_{n}(x)$ 在 $\\mathcal{I}$ 上逐点收敛.函数项级数一致收敛若函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} f_{n}(x)$ 的部分和函数列 $\\{S_{n}(x)\\}$ 一致收敛于 $S(x)$, 即 $\\displaystyle \\lim_{n \\to \\infty} \\sup_{x \\in \\mathcal{I}} \\lvert S_{n}(x) - S(x) \\rvert = 0$, 则称函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} f_{n}(x)$ 在 $\\mathcal{I}$ 上一致收敛. 绝对收敛若 $\\displaystyle \\sum_{n=1}^{\\infty} \\lvert a_{n} \\rvert$ 收敛, 则称数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 绝对收敛.条件收敛若 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 收敛, 但 $\\displaystyle \\sum_{n=1}^{\\infty} \\lvert a_{n} \\rvert$ 发散, 则称数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 条件收敛. 收敛性判别法此处只列举一些常见且通用的收敛性判别法.Cauchy 判别法数项级数收敛, 函数项级数一致收敛的充要条件:对任意 $\\varepsilon &gt; 0$, 存在 $N \\in \\mathbb{N}$, 使得对任意 $n &gt; N$ 和 $p \\in \\mathbb{N}$, 有 \\displaystyle \\lvert a_{n+1} + a_{n+2} + \\cdots + a_{n+p} \\rvert < \\varepsilon \\text{(数项级数)}\\displaystyle \\lvert f_{n+1}(x) + f_{n+2}(x) + \\cdots + f_{n+p}(x) \\rvert < \\varepsilon \\text{(函数项级数)} Abel 判别法 数项级数若数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 收敛, 数列 $\\{b_{n}\\}$ 单调有界, 则级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}b_{n}$ 收敛. 函数项级数若函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}(x)$ 在 $\\mathcal{I}$ 上一致收敛, 且函数列 $\\{v_{n}(x)\\}$ 对每一个 $x \\in \\mathcal{I}$ 都关于 $n$ 单调, 且在 $\\mathcal{I}$ 上一致有界, 则级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}(x)v_{n}(x)$ 在 $\\mathcal{I}$ 上一致收敛. Dirichlet 判别法 数项级数若部分和数列 $\\{S_{n}\\}$ 有界, 数列 $\\{b_{n}\\}$ 单调趋于 $0$, 则级数 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}b_{n}$ 收敛. 函数项级数若部分和函数列 $\\{S_{n}(x)\\}$ 一致有界, 函数列 $\\{v_{n}(x)\\}$ 对于每一个固定的 $x \\in \\mathcal{I}$ 都关于 $n$ 单调, 且在 $\\mathcal{I}$ 上一致趋于 $0$, 则级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}(x)v_{n}(x)$ 在 $\\mathcal{I}$ 上一致收敛. 收敛级数的性质收敛数项级数的性质加法结合律若 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 收敛, 则在它的求和表达式中任意添加或删除括号, 所得到的级数仍然收敛, 且其和不变. 加法交换律若 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 绝对收敛, 则其任意更序级数也绝对收敛, 且其和不变; 若 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 条件收敛, 则存在更序级数条件收敛, 且其和不变. Cauchy 定理若 $\\displaystyle \\sum_{n=1}^{\\infty} a_{n}$ 和 $\\displaystyle \\sum_{n=1}^{\\infty} b_{n}$ 分别绝对收敛于 $A$ 和 $B$, 则 $a_{k}b_{l} \\ (k,l = 1,2, \\cdots)$ 按任意方式相加得到的级数都绝对收敛于 $AB$. 收敛函数项级数的性质逐项积分定理设 $u_{n} \\in R([a,b])$, 且函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}(x)$ 在 $[a,b]$ 上一致收敛于 $S(x)$, 则 $S \\in R([a,b])$ 且 \\int_{a}^{b} S(x) \\, dx = \\sum_{n=1}^{\\infty} \\int_{a}^{b} u_{n}(x) \\, dx.连续性定理设 $u_{n} \\in C(\\mathcal{I})$, 且函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}(x)$ 在 $\\mathcal{I}$ 上一致收敛于 $S(x)$, 则 $S \\in C(\\mathcal{I})$, 即对任意 $x_{0} \\in \\mathcal{I}$, 有 \\lim_{x \\to x_{0}} S(x) = \\lim_{x \\to x_{0}} \\sum_{n=1}^{\\infty} u_{n}(x) = \\sum_{n=1}^{\\infty} \\lim_{x \\to x_{0}} u_{n}(x) = S(x_{0}).逐项求导定理设 $u_{n} \\in C^{1}(\\mathcal{I})$, 且函数项级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}(x)$ 在 $\\mathcal{I}$ 上逐点收敛于 $S(x)$, 且级数 $\\displaystyle \\sum_{n=1}^{\\infty} u_{n}’(x)$ 在 $\\mathcal{I}$ 上一致收敛于 $\\sigma(x)$, 则 $S \\in C^{1}(\\mathcal{I})$ 在区间 $\\mathcal{I}$ 上可导, 且 S'(x) = \\left( \\sum_{n=1}^{\\infty} u_{n}(x) \\right)' = \\sum_{n=1}^{\\infty} u_{n}'(x) = \\sigma(x)."},{"title":"均方极限","path":"/wiki/Stochastic_Process/均方极限.html","content":"二阶矩随机变量 二阶矩随机变量称概率空间 $(\\Omega, F, P)$ 上具有二阶矩的随机变量为二阶矩随机变量, 其全体记为 $H$. $H$ 是完备的线性赋范空间, 也是完备的内积空间. 因而可在 $H$ 上建立极限, 连续, 可积, 可导等概念. 均方极限 均方极限设 $\\{ X(t), t\\in T \\}$ 是二阶矩过程, $X \\in H, t_{0} \\in T$, 如果\\lim_{n \\to \\infty} \\mathbb{E} \\lvert X(t_{n}) - X \\rvert^{2} = 0则称当 $t \\to t_{0}$ 时, $X(t)$ 依均方收敛于 $X$; 或称 $X$ 为当 $t \\to t_{0}$ 时 $\\{ X(t), t\\in T \\}$ 的均方极限, 记为 $X(t) \\overset{M}{\\to} X$. 均方收敛的性质:性质1均方极限在概率1下唯一, 即若 $X(t) \\overset{M}{\\to} X$, $X(t) \\overset{M}{\\to} Y$, 则 $P\\{ X=Y \\} = 1$. 性质2均方收敛强于依概率收敛, 即若 $X(t) \\overset{M}{\\to} X$, 则 $X(t) \\overset{P}{\\to} X$. 性质3若 $\\{ X_{n} \\} \\overset{M}{\\to} X$, 则\\lim_{n \\to \\infty} \\mathbb{E} \\left[ X_{n} \\right] = \\mathbb{E} \\left[ \\lim_{n \\to \\infty} X_{n} \\right] = \\mathbb{E} \\left[ X \\right] 性质4若 $\\{ X_{n} \\} \\overset{M}{\\to} X$, $\\{ Y_{n} \\} \\overset{M}{\\to} Y$, 则\\begin{align}& \\lim_{n \\to \\infty} \\mathbb{E} \\left[ aX_{n} + bY_{n} \\right] = aX + bY \\\\& \\lim_{\\substack{n \\to \\infty \\\\ m \\to \\infty}} \\mathbb{E} \\left[ X_{n} Y_{m} \\right] = \\mathbb{E} \\left[ X Y \\right] \\\\\\end{align}特别地,\\begin{align}\\lim_{n \\to \\infty} \\mathbb{E} \\lvert X_{n} \\rvert^{2} = \\mathbb{E} \\lvert X \\rvert^{2} \\\\\\lim_{n \\to \\infty} \\mathbb{D} \\left[ X_{n} \\right] = \\mathbb{D} \\left[ X \\right]\\end{align} 性质5设 $\\{ a_{n} \\}$ 为普通数列, 且 $\\lim_{n \\to \\infty} a_{n} = 0$, $X$ 为二阶矩随机变量, 则\\lim_{n \\to \\infty} \\mathbb{E} \\left[ a_{n} X \\right] = 0 性质6设 $\\lim_{n \\to \\infty} X_{n} = X$, $f(u)$是一确定函数, 且满足 Lipschitz 条件, 即 $\\lvert f(u) - f(v) \\rvert \\leq M \\lvert u - v \\rvert$, 其中 $M \\gt 0$ 为常数, 则\\lim_{n \\to \\infty} f(X_{n}) = f(X) 性质7 Cauchy 准则Cauchy 准则若 $\\{ X_{n} \\} \\subset H$, 则 $\\{ X_{n} \\}$ 均方收敛的充要条件是: 对任意 $\\varepsilon \\gt 0$, 存在 $N \\in \\mathbb{N},\\ s.t.\\ \\forall n, m \\gt N$, 有\\mathbb{E} \\lvert X_{n} - X_{m} \\rvert^{2} \\lt \\varepsilon 性质8 均方收敛准则均方收敛准则若 $\\{ X_{n} \\} \\subset H$, 则 $\\{ X_{n} \\}$ 均方收敛的充要条件是: 对任意 $\\varepsilon \\gt 0$, 存在 $N \\in \\mathbb{N},\\ s.t.\\ \\forall n, m \\gt N$, 有\\lim_{\\substack{n \\to \\infty \\\\ m \\to \\infty}} \\mathbb{E} \\left[ \\bar{X}_{m} \\bar{X}_{n} \\right] = c其中 $c$ 为常数, 且 $c \\lt +\\inf$. 性质9 均方大数定律均方大数定律若 $\\{ X_{n} \\}$ 是独立同分布的二阶矩随机变量序列, 且 $\\mathbb{E} \\lvert X_{k} \\rvert - \\mu, k=1,2,\\cdots$, 则\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^{n} X_{k} = \\mu 均方连续 均方连续设 $\\{ X(t), t\\in T \\}$ 是二阶矩过程, $t_{0} \\in T$, 如果 $\\lim_{t \\to t_{0}} X(t) = X(t_{0})$, 则称 $\\{ X(t), t\\in T \\}$ 在 $t_{0}$ 处均方连续.若 $\\forall t \\in T$, $\\{ X(t), t\\in T \\}$ 在 $t$ 处均方连续, 则称 $\\{ X(t), t\\in T \\}$ 是均方连续的, 或称 $X(t)$ 是均方连续的. 均方连续的性质:性质1设 $\\{ X(t), t \\in T \\}$ 是二阶矩过程, $R_{X}(s, t)$ 是其相关函数, 则 $\\{ X(t) \\}$ 均方收敛的冲要条件是 $\\forall t \\in T,\\ R_{X} (s, t)$ 在 $(t, t)$ 处均方连续. 性质2设 $\\{ X(t), t \\in T \\}$ 是二阶矩过程, $R_{X}(s, t)$ 是其相关函数, 则 $R_{X}(s, t)$ 在整个 $T \\times T$ 上连续的冲要条件是 $\\forall t \\in T,\\ R_{X} (s, t)$ 在 $(t, t)$ 处均方连续. 性质3若 $\\{ X(t), t \\in T \\}$ 是均方连续的, 则其均值函数 $m_{X}(t)$ 和方差函数 $D_{X}(t)$ 也是连续的. 均方导数 均方导数设 $\\{ X(t), t\\in T \\}$ 是二阶矩过程, $t_{0} \\in T$, 如果均方极限\\lim_{h \\to 0} \\frac{X(t_{0}+ \\Delta t) - X(t_{0})}{\\Delta t}存在, 则称其为 $X(t)$ 在 $t_{0}$ 处均方可导, 其极限称为 $X(t)$ 在 $t_{0}$ 处的均方导数, 记为 $X^{\\prime}(t_{0})$.若 $\\forall t \\in T$, $X(t)$ 在 $t$ 处均方可导, 则称 $\\{ X(t), t\\in T \\}$ 是均方可导的. 广义二阶导数设 $f(s,t)$ 是普通二元函数, 称 $f(s,t)$ 在 $t$ 处的广义二阶可导, 如果下列极限存在:\\lim_{\\substack{h \\to \\infty \\\\ k \\to \\infty}} \\frac{f(s+h, t+k) - f(s+h, t) - f(s, t+k) + f(s, t)}{hk}则称其为 $f(s,t)$ 在 $(s,t)$ 处的广义二阶导数. 均方导数的性质:性质1 均方可导充要条件设 $\\{ X(t), t \\in T \\}$ 是二阶矩过程, $t_{0} \\in T$, 则 $X(t)$ 在 $t_{0}$ 处均方可导的充要条件是: $\\forall t \\in T$, $R_{X}(s, t)$ 在 $(s,t)$ 处广义二阶可导. 性质2 均方可导充要条件设 $\\{ X(t), t \\in T \\}$ 是二阶矩过程, $t_{0} \\in T$, 则 $X(t)$ 在 $t_{0}$ 处均方可导的充要条件是: $\\forall t \\in T$, $R_{X}(s, t)$ 关于 $s,t$ 的一阶偏导数在 $(t_{0}, t_{0})$ 处存在, 且二阶混合偏导数在 $(s,t)$ 处存在且连续. 性质3若 $\\{ X(t), t \\in T \\}$ 是均方可导的, 则 $\\{ X(t), t \\in T \\}$ 均方连续. 性质4若 $\\{ X(t), t \\in T \\}$ 是均方可导的, 则其均方导数在概率1下唯一. 性质5若 $\\{ X(t), t \\in T \\}$ 是均方可导的, 且 $\\forall t \\in T$, $X^{\\prime}(t)=0$, 则 $X(t)$ 以概率1为一随机变量. 性质6若 $\\{ X(t), t \\in T \\}$ 是均方可导的, $X$ 为二阶矩随机变量, 则 $\\left[ X(t)+X \\right]^{\\prime} = X^{\\prime}(t)$ 性质7若 $\\{ X(t), t \\in T \\}$ 是均方可导的, 则:$m_{X^{\\prime}}(t) = m^{\\prime}_{X}(t)$$R_{X^{\\prime}X}(s,t) = \\frac{\\partial}{\\partial s} R_{x}(s,t)$$R_{XX^{\\prime}}(s,t) = \\frac{\\partial}{\\partial t} R_{x}(s,t)$$R_{X^{\\prime}}(s,t) = \\frac{\\partial^{2}}{\\partial s \\partial t} R_{X}(s,t)$ 性质8若 $\\{ X(t), t \\in T \\}$, $\\{ Y (t), t \\in T \\}$ 都是均方可导的, $a,b$ 为任意常数, 则:$\\{ aX(t) + bY(t), t \\in T \\}$ 也均方可导, 且 $(aX(t)+bY(t))^{\\prime} = aX^{\\prime}(t) + bY^{\\prime}(t)$ 性质9若 $\\{ X(t), t \\in T \\}$ 是均方可导的, $f(t)$ 是 $T$ 上的普通可导函数, 则 $\\{ f(t)X(t), t \\in T \\}$ 也是均方可导的, 且 $(f(t)X(t))^{\\prime} = f^{\\prime}(t)X(t) + f(t)X^{\\prime}(t)$ 均方积分 均方积分给定随机过程 $\\{ X(t), a \\leq t \\leq b \\}$, $f(t)$ 为 $[a,b]$ 上的普通函数. 任意插入 $n-1$ 个分点 $a=t_{0} \\lt t_{1} \\lt \\cdots \\lt t_{n}=b$, 作和式\\sum_{k=1}^{n} f(u_{k}) X(u_{k}) (t_{k} - t_{k-1})其中 $u_{k} \\in [t_{k-1}, t_{k}]$. 记 $\\Delta = \\max_{1 \\leq k \\leq n} (t_{k} - t_{k-1})$, 若均方极限\\lim_{\\Delta \\to 0} \\sum_{k=1}^{n} f(u_{k}) X(u_{k}) (t_{k} - t_{k-1})存在, 且极限与区间的分法, $u$, $k$ 的取值无关, 则称其为 $f(t)X(t)$ 在 $[a,b]$ 上的黎曼均方积分, 简称为均方积分, 记为\\int_{a}^{b} f(t) X(t) dt特别地, 若 $f(t) \\equiv 1$, 有\\int_{a}^{b} X(t) dt = \\lim_{\\Delta \\to 0} \\sum_{k=1}^{n} X(u_{k}) (t_{k} - t_{k-1})为随机过程 $\\{ X(t), a \\leq t \\leq b \\}$ 在 $[a,b]$ 上的均方积分. 此时, 称 $X(t)$ 在 $[a,b]$ 上是均方可积的. 均方积分的性质:性质1 充要条件设 $\\{ X(t), a \\leq t \\leq b \\}$ 是二阶矩过程, 则 $\\{ f(t)X(t), t \\in [a,b] \\}$ 在 $[a,b]$ 上的均方积分存在的充要条件是下列二重积分存在:\\int_{a}^{b} \\int_{a}^{b} f(s) \\overline{f(t)} R_{X}(s,t) ds dt且有\\mathbb{E} \\lvert \\int_{a}^{b} f(t) X(t) dt \\rvert^{2} = \\int_{a}^{b} \\int_{a}^{b} \\int_{a}^{b} \\int_{a}^{b} f(s) \\bar{f(t)} R_{X}(s,t) ds dt 性质2若 $\\{ X(t), a \\leq t \\leq b \\}$ 是均方可积的, 则其均方积分在概率1下唯一. 性质3若 $\\{ X(t), a \\leq t \\leq b \\}$ 是均方连续的, 则其在 $[a,b]$ 上是均方可积的. 性质4若二阶矩过程 $\\{ X(t), a \\leq t \\leq b \\}$, $\\{ Y(t), a \\leq t \\leq b \\}$ 均方可积, 则\\int_{a}^{b} \\left[ \\alpha X(t) + \\beta Y(t) \\right] dt = \\alpha \\int_{a}^{b} X(t) dt + \\beta \\int_{a}^{b} Y(t) dt 性质5若 $\\{ f(t)X(t), a \\leq t \\leq b \\}$ 在 $[a,b]$ 上的均方积分存在, 则 $\\{ f(t)X(t), a \\leq t \\leq b \\}$ 在 $[a,c]$ 和 $[c,b]$ 上的均方积分也存在, 且有\\int_{a}^{b} f(t) X(t) dt = \\int_{a}^{c} f(t) X(t) dt + \\int_{c}^{b} f(t) X(t) dt 性质6\\mathbb{E} \\left[ \\int_{a}^{b} f(t) X(t) dt \\right] = \\int_{a}^{b} \\mathbb{E} \\left[ f(t) m_{X}(t) \\right] dt 性质7若随机过程 $\\{ X(t), a \\leq t \\leq b \\}$ 在 $[a,b]$ 上均方可导, 则\\int_{a}^{b} f(t) X^{\\prime}(t) dt = X(b) - X(a) 性质8若随机过程 $\\{ X(t), a \\leq t \\leq b \\}$ 在 $[a,b]$ 上均方连续, 则\\mathbb{E} \\lvert \\int_{a}^{b} X(t) dt \\rvert^{2} \\leq \\left( b-a \\right) \\int_{a}^{b} \\left[ \\mathbb{E} \\lvert X(t) \\rvert^{2} \\right]^{\\frac{1}{2}} dt \\leq \\left( b-a \\right)^2 \\max_{a \\leq t \\leq b} \\mathbb{E} \\lvert X(t) \\rvert^{2} 性质9若随机过程 $\\{ X(t), a \\leq t \\leq b \\}$ 在 $[a,b]$ 上均方连续, 则其均方不定积分 $Y(t), t \\in T$ 在 $[a,b]$ 上均方可导, 且 \\begin{align} P\\{ Y^{\\prime}(t) = X(t) \\} = 1 \\\\ m_{Y}(t) = \\int_{a}^{t} m_{X}(s) ds \\\\ R_{Y}(s,t) = \\int_{a}^{s} \\int_{a}^{t} R_{X}(u,v) du dv \\\\ C_{Y}(s,t) = \\int_{a}^{s} \\int_{a}^{t} C_{X}(u,v) du dv\\end{align} 正太过程的随机分析 性质1$n$ 维正太随机变量序列的均方极限也是 $n$ 维正太随机变量. 性质2设 $\\{ X(t) = \\{ X_{1}(t), X_{2}(t), \\cdots, X_{n}(t) \\}, t \\in T \\}$ 是 $n$ 维正太随机变量序列, $t_{0} \\in T$, 若\\lim_{t \\to t_{0}} X(t) = X_{k},\\quad k = 1,2,\\cdots,n则 $X(t)$ 为 $n$ 维随机变量 性质3若实正太过程 $\\{ X(t), t \\in T \\}$ 均方可导, 则其均方导数过程 $\\{ X^{\\prime}(t), t \\in T \\}$ 也然是正太过程. 性质4若实正太过程 $\\{ X(t), t \\in T \\}$ 均方连续, 则其均方不定积分过程 $\\{ Y(t), t \\in T \\}$ 也然是正太过程."},{"title":"马尔可夫链","path":"/wiki/Stochastic_Process/马尔可夫链.html","content":"马尔可夫链 马尔可夫链给定随机变量序列 $\\{ X_{n}, n=1,2,\\cdots \\}$, 如果对 $1 \\leq n_{1} &lt; n_{2} &lt; \\cdots &lt; n_{k}$, 及 $i_{1}, i_{2}, \\cdots, i_{k} \\in E$, 有P \\{ X_(n_{k+1}) = i_{k+1} \\vert X_{n_{1}} = i_{1}, X_{n_{2}} = i_{2}, \\cdots, X_{n_{k}} = i_{k} \\} = P \\{ X_{n_{k+1}} = i_{k+1} \\vert X_{n_{k}} = i_{k} \\}则称 $\\{ X_{n}, n=1,2,\\cdots \\}$ 为马尔可夫链. 称 $E = \\{ 1, 2, \\cdots, m \\}$ 为状态空间. 转移概率设 $\\{ X_{n}, n=1,2,\\cdots \\}$ 为马尔可夫链, 称p_{i, j}(n, k) = P \\{ X_{n+k} = j \\vert X_{n} = i \\}为 $\\{ X_{n}, n=1,2,\\cdots \\}$ 在时刻 $n$ 的 $k$ 步转移概率, 称P(n, k) = \\left( p_{i, j}(n, k) \\right) \\quad (i,j \\in E)为 $\\{ X_{n}, n=1,2,\\cdots \\}$ 在时刻 $n$ 的 $k$ 步转移概率矩阵.特别地, 当 $k=1$ 时, 称 $p_{i, j}(n, 1) = p_{i, j}(n)$ 为 $\\{ X_{n} \\}$ 在时刻 $n$ 的转移概率, 称 $P(n) = \\left( p_{i, j}(n) \\right)$ 为 $\\{ X_{n} \\}$ 在时刻 $n$ 的一步转移概率矩阵. 若随机变量序列 $\\{ X_{n}, n=1,2,\\cdots \\}$ 为独立增量过程, 且 $X(0)=0$, 则 $X_{n}$ 为马尔可夫链. 离散参数齐次马氏链 离散参数齐次马氏链设 $\\{ X_{n}, n=1,2,\\cdots \\}$ 为马尔可夫链, 如果其一步转移概率 $p_{i, j}(n,1)$ 恒与起始时刻 $n$ 无关, 记为 $p_{ij}$, 则称 $\\{ X_{n}, n=1,2,\\cdots \\}$ 为离散参数齐次马氏链. 若随机变量序列 $\\{ X_{n}, n=1,2,\\cdots \\}$ 为平稳独立增量过程, 且 $X(0)=0$, 则 $X_{n}$ 为齐次马尔可夫链. 初始分布 &amp; 绝对分布称 $p_i = P(X(0)=i), i \\in E$, 为马氏链 $\\{ X(n) \\}$ 的初始分布, 记为 $\\mathbf{\\tilde{P}}_{0}$.称 $p_j(n) = P(X(n)=j), j \\in E$, 为马氏链 $\\{ X(n) \\}$ 的绝对分布, 记为 $\\mathbf{\\tilde{P}}_{n}$. 性质: C-K 方程: $p_{i,j} (k+l) = \\sum_{r \\in E} p_{i,r}(k) p_{r,j}(l)$ 平稳性: $P(n) = P^{n}$ 绝对分布由初始分布和转移概率确定, 且满足 $\\mathbf{\\tilde{P}}_{n} = \\mathbf{\\tilde{P}}_{0} P^n$ 齐次马氏链的有限维分布由初始分布和转移概率确定, 且满足 P \\{ X(n_{1}) = i_{1}, \\cdots X(n_{k}) = i_{k} \\} = \\sum_{i \\in E} p_{i} \\cdot p_{i,i_{1}} \\cdots p_{i_{k-1},i_{k}} 离散齐次马氏链的状态分类 状态转移概率图形: 可达 &amp; 互通可达设 $i,j \\in E$, 若 $\\exists n \\geq 1$, 使 $p_{i,j}(n) \\gt 0$, 则称状态 $i$ 可达状态 $j$, 记为 $i \\rightarrow j$.互通若 $i \\rightarrow j$ 且 $j \\rightarrow i$, 这称状态 $i$ 与状态 $j$ 互通, 记为 $i \\leftrightarrow j$. 首达时刻 &amp; 首达概率 &amp; 常返状态 &amp; 周期首达时刻从状态 $i$ 出发，首次到达状态 $j$ 的时刻 $T_{ij}$ 称为首达时刻.首达概率从状态 $i$ 出发, 经过 $n$ 步首次到达状态 $j$ 的概率为f_{ij} = P \\{ T_{ij}=n \\vert X(m)=i \\}称为首达概率.常返状态设 $i \\in E$, 若 $f_{ii}=1$, 则称状态 $i$ 为常反状态; 否则, 称为非常返状态.周期记 $d(i)$ 为满足 $p_{ij}(n) \\gt 0$ 的所有 $n$ 的最大公约数. 若 $d(i) \\gt 1$, 称 $i$ 有周期 $d(i)$, 否则称 $i$ 为非周期的。 闭集 &amp; 吸收状态 &amp; 可约集闭集设 $C$ 是 $E$ 的一个子集, 如果 $\\forall i \\in C, \\forall j ot \\in C, \\forall n \\geq 0$, 有 $p_{ij}^{(n)} = 0$, 则称 $C$ 是闭集.吸收状态设 $i \\in E$, 如果状态子集 $\\{ i \\}$ 是闭集, 则称状态 $i$ 为吸收状态.可约集设 是闭集, 如果 $C$ 中不再含有任何非空真闭子集, 则称 $C$ 是不可约闭集; 如果状态空间 $E$ 是不是约的，那么该马尔可夫链是不可约的, 否则称为可约的. 性质: $p_{ij}(n) = \\sum_{m = 1}^{n} f_{ij} p_{jj}(n-m)$ $f_{ij} \\gt 0 \\Leftrightarrow i \\to j$ $i$ 是常反状态 $\\Leftrightarrow$ $\\displaystyle\\sum_{n=1}^{\\infty} p_{ii}(n) = \\infty$; $i$ 是非常反状态 $\\Leftrightarrow$ $\\displaystyle \\sum_{n=1}^{\\infty} p_{ii}(n) = +\\infty$ 若 $i \\leftrightarrow j$, 则 $i$ 与 $j$ 同为常反状态或同为非常反状态, 若它们均为常返态, 则它们同为正常返态或零常返态.、 齐次马尔科夫链的状态空间 $S$ 可唯一地分解成有限或可列无限多个互不相交的状态子集之并, 即 $S = D \\cup C_{1} \\cup C_{2} \\cup \\cdots$, 其中 $D$ 是吸收状态集, $C_{1}, C_{2}, \\cdots$ 是不可约闭集. 离散参数齐次马氏链的遍历性 遍历态离散参数齐次马氏链 $\\{ X_{n}, n=1,2,\\cdots \\}$ 若状态 $i$ 为正常返且为非周期, 则称 $i$ 为遍历态。 离散参数齐次马氏链的平稳分布离散参数齐次马氏链 $\\{ X_{n}, n=1,2,\\cdots \\}$ 若存在 $\\{ v_{j}, j \\in E \\}$, 满足条件:$v_{j} \\geq 0, j \\in E$$\\displaystyle \\sum_{j \\in E} v_{j} = 1$$\\displaystyle v_{j} = \\sum_{i \\in E} v_{i} p_{ij}$则称 $\\{ X_{n} \\}$ 是平稳的, $\\{ v_{j}, j \\in E \\}$ 为离散参数齐次马氏链 $\\{ X_{n} \\}$ 的平稳分布. 极限分布若离散参数齐次马氏链 $\\{ X_{n}, n=1,2,\\cdots \\}$ 的绝对分布存在, 即 $\\lim_{n \\to \\infty} \\pi_{j}(n) = \\pi_{j}^{} \\ (j \\in S)$ 存在, 则称 $\\pi^{} = \\{ \\pi_{0}, \\pi_{1}, \\cdots \\}$ 为 $\\{ X_{n} \\}$ 的极限分布."},{"title":"Taxonomizing local versus global structure in neural network loss landscapes","path":"/wiki/Loss_Landscape/Applications/Taxonomizing local versus global structure in neural network loss landscapes.html","content":"Taxonomy"},{"title":"Hilbert-Schmidt independence criterion (HSIC)","path":"/wiki/Loss_Landscape/Preliminaries/Hilbert-Schmidt independence criterion (HSIC).html","content":"对于两个随机变量 $X$ 和 $Y$, 他们的独立性可以通过他们的联合分布 $P_{XY}$ 和边缘分布 $P_{X}P_{Y}$ 来判断. 但是在实际应用中, 我们往往只有样本, 而没有真实的分布. 这时候我们可以通过样本来估计他们的独立性. 一种常用的方法是 Hilbert-Schmidt independence criterion (HSIC). Setup设 $X = \\{x_{1}, x_{2}, \\ldots, x_{n}\\}$ 和 $Y = \\{y_{1}, y_{2}, \\ldots, y_{n}\\}$ 是两个样本集合, 其中 $x_{i} \\in \\mathcal{X}$, $y_{i} \\in \\mathcal{Y}$, $i = 1, 2, \\ldots, n$. $f: \\mathcal{X} \\rightarrow \\mathbb{R}$ 和 $g: \\mathcal{Y} \\rightarrow \\mathbb{R}$ 是两个函数. 我们希望通过 $f$ 和 $g$ 来判断 $X$ 和 $Y$ 是否独立. x,y 相互独立, 即 $p(x,y) = p(x)p(y)$ 的充要条件如下:\\begin{align} C[f, g] &= \\iint p(x, y) f(x) g(y) \\mathrm{d}x \\mathrm{d}y - \\iint p(x) p(y) f(x) g(y) \\mathrm{d}x \\mathrm{d}y \\\\ &= \\mathbb{E}_{(x,y) \\sim p(x,y)}[f(x)g(y)] - \\mathbb{E}_{x \\sim p(x)}[f(x)]\\mathbb{E}_{y \\sim p(y)}[g(y)] = 0\\end{align}\\tag{1}\\label{eq1} 这个结论显然不难理解。有意思的是，等号右边是采样的形式，也就是说我们将这个指标转化为了采样的形式，避免了直接估算概率密度。 这样一来, 我们就有一个判断独立性的方法: 选取”足够多”的 $f, g$, 然后计算 L_{H} = \\sum_{f,g} (C[f,g])^2 \\tag{2}\\label{eq2}如果 $L_{H} = 0$, 那么我们就可以认为 $X$ 和 $Y$ 是独立的. HSIC进一步推导 $(C[f,g])^2$: \\begin{align} (C[f, g])^2 &= (\\mathbb{E}_{(x,y) \\sim p(x,y)}[f(x)g(y)] - \\mathbb{E}_{x \\sim p(x)}[f(x)]\\mathbb{E}_{y \\sim p(y)}[g(y)])^2 \\\\ &= \\mathbb{E}_{(x,y) \\sim p(x,y)}[f(x)g(y)]^2 + \\mathbb{E}_{x \\sim p(x)}[f(x)]^2 \\mathbb{E}_{y \\sim p(y)}[g(y)]^2 \\\\ &\\quad - 2\\mathbb{E}_{(x,y) \\sim p(x,y)}[f(x)g(y)]\\mathbb{E}_{x \\sim p(x)}[f(x)]\\mathbb{E}_{y \\sim p(y)}[g(y)] \\\\ &= \\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y)}[f(x_{1})g(y_{1})] \\cdot \\mathbb{E}_{(x_{2},y_{2}) \\sim p(x,y)}[f(x_{2})g(y_{2})] \\\\ &\\quad + \\mathbb{E}_{x_{1} \\sim p(x)}[f(x_{1})] \\cdot \\mathbb{E}_{x_{2} \\sim p(x)}[f(x_{2})] \\cdot \\mathbb{E}_{y_{1} \\sim p(y)}[g(y_{1})] \\cdot \\mathbb{E}_{y_{2} \\sim p(y)}[g(y_{2})] \\\\ &\\quad - 2\\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y)}[f(x_{1})g(y_{1})]\\mathbb{E}_{x_{2} \\sim p(x)}[f(x_{2})]\\mathbb{E}_{y_{2} \\sim p(y)}[g(y_{2})] \\\\ &= \\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), (x_{2},y_{2}) \\sim p(x,y)}[f(x_{1})f(x_{2})g(y_{1})g(y_{2})] \\\\ &\\quad + \\mathbb{E}_{x_{1} \\sim p(x), x_{2} \\sim p(x), y_{1} \\sim p(y), y_{2} \\sim p(y)}[f(x_{1})f(x_{2})g(y_{1})g(y_{2})] \\\\ &\\quad - 2\\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), x_{2} \\sim p(x), y_{2} \\sim p(y)}[f(x_{1})f(x_{2})g(y_{1})g(y_{2})] \\\\ \\end{align} \\tag{3}\\label{eq3}接下来, 我们引入核函数 $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$, 我们知道核函数具有以下性质: 对称性: $k(x, y) = k(y, x)$; 正定性: $\\displaystyle \\iint k(x, y) f(x) f(y) \\mathrm{d}x \\mathrm{d}y \\geq 0$. 核函数的所有特征函数 $\\phi_{i}(x)$ 构成 $\\mathcal{X}$ 上的一组正交基. Mercer 定理: 如果 $k$ 是一个连续核, 则 $k$ 可以表示为: $\\displaystyle k(x, y) = \\sum_{i=1}^{\\infty} \\lambda_{i} \\phi_{i}(x) \\phi_{i}(y)$ 我们可以用核函数代替式 $\\eqref{eq3}$ 中的 $f$ 和 $g$:我们的目标是考察 $f$ 和 $g$ 的独立性, 注意到用 $L_{H} = \\sum_{i,j} (C[\\phi_{i}, \\phi_{j}])^2$ 和 L_{H} = \\sum_{i,j} \\alpha_{i}\\beta_{j} \\cdot (C[k(x, \\cdot), k(y, \\cdot)])^2\\ \\tag{4}\\label{eq4}来描述独立性是等价的. 于是我们可以用核函数来描述独立性.将 $\\eqref{eq4}$ 展开: \\begin{align} L_{H} &= \\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), (x_{2},y_{2}) \\sim p(x,y)}[\\sum_{i,j} \\alpha_{i}\\beta_{j} \\phi_{i}(x_{1})\\phi_{i}(x_{2})\\phi_{j}(y_{1})\\phi_{j}(y_{2})] \\\\ &\\quad + \\mathbb{E}_{x_{1} \\sim p(x), x_{2} \\sim p(x), y_{1} \\sim p(y), y_{2} \\sim p(y)}[\\sum_{i,j} \\alpha_{i}\\beta_{j} \\phi_{i}(x_{1})\\phi_{i}(x_{2})\\phi_{j}(y_{1})\\phi_{j}(y_{2})] \\\\ &\\quad - 2\\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), x_{2} \\sim p(x), y_{2} \\sim p(y)}[\\sum_{i,j} \\alpha_{i}\\beta_{j} \\phi_{i}(x_{1})\\phi_{i}(x_{2})\\phi_{j}(y_{1})\\phi_{j}(y_{2})] \\\\ &= \\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), (x_{2},y_{2}) \\sim p(x,y)}[K_{X}(x_{1}, x_{2}) K_{Y}(y_{1}, y_{2})] \\\\ &\\quad + \\mathbb{E}_{x_{1} \\sim p(x), x_{2} \\sim p(x), y_{1} \\sim p(y), y_{2} \\sim p(y)}[K_{X}(x_{1}, x_{2}) K_{Y}(y_{1}, y_{2})] \\\\ &\\quad - 2\\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), x_{2} \\sim p(x), y_{2} \\sim p(y)}[K_{X}(x_{1}, x_{2}) K_{Y}(y_{1}, y_{2})] \\\\ \\end{align} \\tag{5}\\label{eq5}矩阵形式以线性核 $K_{X}(x_{1}, x_{2}) = x_{1}^{T}x_{2}$ 和 $K_{Y}(y_{1}, y_{2}) = y_{1}^{T}y_{2}$ 为例, $K_{X}$ 和 $K_{Y}$ 为 $n \\times n$ 的矩阵.式 $\\eqref{eq5}$ 中第一项可以写成: \\begin{align} & \\mathbb{E}_{(x_{1},y_{1}) \\sim p(x,y), (x_{2},y_{2}) \\sim p(x,y)}[K_{X}(x_{1}, x_{2}) K_{Y}(y_{1}, y_{2})] \\\\ &= \\frac{1}{n^{2}} \\sum_{i}^{n} \\sum_{j}^{n} [K_{X}(x_{i}, x_{j}) K_{Y}(y_{i}, y_{j})] \\\\ &= \\frac{1}{n^{2}} tr (K_{X} K_{Y}) \\end{align}同理, 第二项和第三项也可以写成: $\\displaystyle \\frac{1}{n^{4}} tr (K_{X} \\mathbb{1} K_{Y} \\mathbb{1})$ 和 $\\displaystyle \\frac{2}{n^{3}} tr (K_{X} K_{Y} \\mathbb{1})$. 故 $L_{H}$ 可以写成: \\begin{align} L_{H} &= \\frac{1}{n^{4}} tr (K_{X} K_{Y}) + \\frac{1}{n^{2}} tr (K_{X} \\mathbb{1} K_{Y} \\mathbb{1}) - \\frac{2}{n^{3}} tr (K_{X} K_{Y} \\mathbb{1}) \\\\ &= \\frac{1}{n^{2}} tr (H K_{X} H K_{Y}) \\\\ \\end{align} \\tag{6}\\label{eq6}其中 $H = I - \\frac{1}{n} \\mathbb{1} \\mathbb{1}^{T}$, $\\mathbb{1}$ 是中心矩阵, 即 H = \\begin{pmatrix} 1 - \\frac{1}{n} & -\\frac{1}{n} & \\cdots & -\\frac{1}{n} \\\\ -\\frac{1}{n} & 1 - \\frac{1}{n} & \\cdots & -\\frac{1}{n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ -\\frac{1}{n} & -\\frac{1}{n} & \\cdots & 1 - \\frac{1}{n} \\end{pmatrix}注意到, 式 $\\eqref{eq6}$ 是一个有偏估计, 故改写为无偏估计, 即最终的 HSIC 为: HSIC = \\frac{1}{(n-1)^2} tr (H K_{X} H K_{Y})\\tag{7}\\label{eq7} 注: $X$ 和 $Y$ 独立 $\\Leftrightarrow$ $HSIC = 0$. HSIC 相当于联合分布 $P_{XY}$ 和边缘分布 $P_{X}P_{Y}$ 之间的最大均值差异. 具有特定的核函数, HSIC 相当于距离协方差."},{"title":"Reproducing Kernel Hilbert Space (RKHS)","path":"/wiki/Loss_Landscape/Preliminaries/Reproducing Kernel Hilbert Space (RKHS).html","content":"Kernel设 设 $\\mathcal{F}$ 是函数集 $f: \\mathcal{X} \\rightarrow \\mathbb{R}$ 的空间.核函数 (Kernel)二元函数 $K(x, y): \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ 如果满足:对称性: $K(x, y) = K(y, x)$;正定性: $\\displaystyle \\iint K(x, y) f(x) f(y) \\mathrm{d}x \\mathrm{d}y \\geq 0$.则称 $K$ 是 $\\mathcal{X}$ 上的核. 特征值和特征函数对于核 $K$, 存在特征值 $\\lambda \\geq 0$ 和特征函数 $\\phi(x)$, 满足:K(x, y) = \\int_{i=1}^{\\infty} \\lambda \\phi(x) \\phi(y) dy 易证: 一个核的任意特征函数是正交的, 即 $\\displaystyle &lt;\\phi_{i}(x), \\phi_{j}(x)&gt; =\\int \\phi_{i}(x) \\phi_{j}(x) \\mathrm{d}x = 0$. 特征值分解 (Mercer定理)如果 $K$ 是一个连续核, 则 $K$ 可以表示为:K(x, y) = \\sum_{i=1}^{\\infty} \\lambda_{i} \\phi_{i}(x) \\phi_{i}(y) Reproducing Kernel Hilbert Space 设 $\\mathcal{H}$ 是函数集 $f: \\mathcal{X} \\rightarrow \\mathbb{R}$ 的 Hilbert 空间 对于固定的 $x \\in \\mathcal{X}$, 点 $x$ 的 Dirac evaluation 函数为 $\\delta_{x}: \\mathcal{H} \\rightarrow \\mathbb{R}$, $\\delta_{x}(f) = f(x)$. Reproducing Kernel Hilbert Space (RKHS)如果对于任意 $x \\in \\mathcal{X}$, $\\delta_{x}$ 都是 $\\mathcal{H}$ 上的连续线性函数, 则 $\\mathcal{H}$ 是一个 再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS). 再生核$K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ 如果满足:对于任意 $x \\in \\mathcal{X}$, $K(x, \\cdot) \\in \\mathcal{H}$;对于任意 $x \\in \\mathcal{X}$, $f(x) = \\langle f, K(x, \\cdot) \\rangle_{\\mathcal{H}}$.则称 $K$ 是 $\\mathcal{H}$ 的再生核, 记 $\\Phi(x) = K(x, \\cdot)$. 设 $\\mathcal{H}$ 是一个 $\\mathcal{X}$ 上的 RKHS, 其再生核为 $K$, 则:性质1对 $\\forall x \\in \\mathcal{X}$, $K(x, \\cdot)$ 是 $\\delta_{x}$ 的线性组合, 即 $\\displaystyle K(x, \\cdot) = \\sum_{i=1}^{n} a_{i} \\delta_{x_{i}}$.性质2对 $\\forall x, y \\in \\mathcal{X}$, 有 $K(y, \\cdot)$ $\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)$. 再生核希尔伯特空间的两个重要性质 任意RKHS $\\mathcal{H}$ 的再生核 $K$ 唯一. $\\mathcal{H}$ 是一个RKHS $\\Leftrightarrow$ $\\mathcal{H}$ 有一个再生核."},{"title":"Centered Kernel Alignment (CKA)","path":"/wiki/Loss_Landscape/Methmatic Tools/Centered Kernel Alignment (CKA).html","content":"这是一篇关于 Similarity of Neural Network Representations Revisited 的笔记. Setup $X \\in \\mathbb{R}^{n \\times p_{1}}$, $Y \\in \\mathbb{R}^{n \\times p_{2}}$ 是两个激活矩阵. $n$ 是样本数, $p_{1}$ 和 $p_{2}$ 分别是两个激活矩阵的维度. 不是一般性, 我们假设 $p_{1} \\leq p_{2}$. 我们希望找到一个指标 $s(X, Y)$ 来衡量两个激活矩阵之间的相似性. 该研究认为, 相似性的直观概念和神经网络训练的动态过程都要求 $s$ 满足对正交变换和满足对各向同性缩放是不变的, 而非对可逆线性变换的不变. 可逆线性变换不变性可逆线性变换不变性对于任意满秩矩阵 $A$, $B$, 如果 $s(X, Y) = s(XA, YB)$, 那么就称 $s(X, Y)$ 满足可逆线性变换不变性. Theorem 1设 $X, Y \\in \\mathbb{R}^{n \\times p}$, 对任意 $Z$ 和秩为 $p$ 的矩阵 $A$:rank(X) = rank(Y) = n \\Rightarrow s(X, Z) = s(Y, Z) 可逆线性变换可变性的局限性:文章指出，对可逆线性变换不变的任何相似性指数在表示的维度大于数据点数量时会遇到问题。具体来说，这样的指数对于宽度（即神经元数量）大于或等于数据集大小的任何表示都会给出相同的结果。这在实际应用中可能不是特别有用，因为在深度学习中，很多时候层的宽度会超过样本数量。具体来说, 可逆线性变换不变性会导致关键信息的损失, 如: 方向的缩放, 欧氏距离的一致性. 正交变换不变性正交变换不变性对于任意正交矩阵 $U$, $V$ ($U^{T}U=V^{T}V=I$), 如果 $s(X, Y) = s(XU, YV)$, 那么就称 $s(X, Y)$ 满足可逆线性变换不变性. 正交变换不变性是一个比可逆线性变换不变性更弱的条件. 两者在几何结构方面的对比如下: 可逆线性变换不变性包括旋转, 对称等正交变换;保持了数据的线性属性不变, 但几何结构可能会变. 正交变换不变性包括旋转, 对称等正交变换;保持了数据的几何结构 (标量积, 欧式距离等) 不变. 各向同性缩放不变性正交变换不变性对于任意 $\\alpha ,\\beta \\in \\mathbb{R}^{+}$, 如果 $s(X, Y) = s(\\alpha X, \\beta Y)$, 那么就称 $s(X, Y)$ 满足可逆各向同性缩放不变性. 注意到:由矩阵奇异值的存在性得到:正交变化不变性 $+$ 非各向同性缩放不变性 (各个特征值的重新缩放) $=$ 可逆线性变换不变性. Centered Kernel Alignment (CKA) 我们已经了解了 HSIC :HSIC (K, L) = \\frac{1}{(n-1)^2} tr (H K H L)其中 $K, L$ 是两个核矩阵, $H$ 是中心矩阵. 注意到, HSIC 并不满足可逆线性变换, 故我们将其归一化, 得到 CKA:CKA (K, L) = \\frac{HSIC (K, L)}{\\sqrt{HSIC (K, K)} \\sqrt{HSIC (L, L)}} 以下是两个常见的核矩阵的 CKA: 线性核 $K_{X}(x_{1}, x_{2}) = x_{1}^{T}x_{2}$ CKA (K, L) = \\frac{\\lVert Y^{T}X \\rVert_{F}^{2}}{\\lVert X^{T}X \\rVert_{F} \\lVert Y^{T}Y \\rVert_{F}} RBF 核 $K_{X}(x_{1}, x_{2}) = \\exp(-\\lVert x_{1} - x_{2} \\rVert^{2} / 2\\sigma^{2})$ CKA (K, L) = \\frac{tr(KHLH)}{\\sqrt{tr(KHKH)} \\sqrt{tr(KHLH)}} Related Similarity Indexes 记 $Q_{X}, Q_{Y}$ 是 $X, Y$ 的一组标准正交基, 我们可以得到以下几种相似性度量, 它们的公式和性质如下: Linear Ridgelet为了刻画 $Q_{X}$ 和 $Q_{X}$ 之间的相关性, 一个 Naive 的方式是尝试用 Y 的线性组合来表出 X, 并最大化相关系数. 于是有了以下基于线性回归的度量:R_{LR}^{2} = 1 - \\frac{\\min_{B} \\lVert X-YB \\rVert_{F}^2}{\\lVert X \\rVert_{F}^2} = \\frac{\\lVert Q_{Y}^{T} X \\rVert_{F}^2}{\\lVert X \\rVert_{F}^2} Canonical Correlation Analysis (CCA)CCA 试图用通过最大化 $X$ 的线性组合和 $Y$ 的线性组合之间的相关系数来衡量两个矩阵之间的相关性. 具体来说, CCA 通过以下方式来定义一组相关系数 $\\rho_{1}, \\cdots \\rho_{p_{1}}$:\\begin{align} \\rho_{i} &= \\max_{w_{X}^{i}, w_{Y}^{i}} corr(w_{X}^{i}X, w_{Y}^{i}Y) \\\\ subject \\ to: & \\forall j < i, X w_{X}^{i} \\perp X w_{X}^{j}, Y w_{Y}^{i} \\perp Y w_{Y}^{j}\\end{align}基于以上定义, 我们可以得到以下两种 CCA 的度量方式:R_{CCA}^{2} = \\frac{\\sum_{i=1}^{p_{1}} \\rho_{i}^2}{p_{1}} = \\frac{\\lVert Q_{Y}^{T} Q_{X} \\rVert_{F}^{2}}{p_{1}} \\quad \\text{or} \\quad \\bar{\\rho_{CCA}} = \\frac{\\sum_{i=1}^{p_{1}} \\rho_{i}}{p_{1}} = \\frac{\\lVert Q_{Y}^{T} Q_{X} \\rVert_{*}}{p_{1}}其中 $\\lVert \\cdot \\rVert_{F}$ 和 $\\lVert \\cdot \\rVert_{*}$ 分别是 Frobenius 范数和核范数. Singular Value CCA (SVCCA)CCA 对扰动敏感, 为了提高 CCA 的鲁棒性, SVCCA 通过对 X 和 Y 的截断奇异值分解进行 CCA, 从而得到了一个更稳定的度量方式:\\begin{align} R_{SVCCA}^{2} &= 1 - \\frac{\\sum_{i=1}^{m} \\rho_{i}^2}{m} = \\frac{\\lVert (U_{Y}T_{Y})^{T} U_{X}T_{X} \\rVert_{F}^{2}}{ \\min( \\lVert T_{X} \\rVert_{F}^{2}, \\lVert T_{Y} \\rVert_{F}^{2} ) } \\\\ \\bar{\\rho}_{SVCCA} &= 1 - \\frac{\\sum_{i=1}^{m} \\rho_{i}}{m} = \\frac{\\lVert (U_{Y}T_{Y})^{T} U_{X}T_{X} \\rVert_{*}}{ \\min( \\lVert T_{X} \\rVert_{F}, \\lVert T_{Y} \\rVert_{F} ) } \\end{align}其中 $U_{X}, U_{Y}$ 是 $X, Y$ 的左奇异矩阵, $m$ 是截断数, $T_{X}, T_{Y}$ 是 $X, Y$ 的截断单位矩阵, 用来选择左奇异向量. Projection Weighted CCA (PWCCA)另一种提高 CCA 鲁棒性的方式是 PWCCA. PWCCA 注意到 占原始输出比重较大的CCA vector $\\rho^{i}$ 对于潜在的representation更重要. 故相比于 SVCCA 直接取算数平均, PWCCA 通过给每一个 $\\rho^{i}$ 添加权重 $\\alpha^{i}$ 来计算加权平均值:\\rho_{PWCCA} = \\frac{\\sum_{i=1}^{m} \\alpha^{i} \\rho^{i}}{\\sum_{i=1}^{m} \\alpha^{i}}\\quad \\text{where} \\quad\\alpha^{i} = \\sum_{j} \\lvert \\langle X w_{X}^{i}, x_{j} \\rangle \\rvert基于 $\\rho_{PWCCA}$, 我们可以得到 PWCCA 的度量方式:R_{PWCCA}^{2} = \\rho_{PWCCA} = \\frac{\\sum_{i=1}^{m} \\alpha^{i} \\rho^{i}}{\\sum_{i=1}^{m} \\alpha^{i}} Neuron Alignment Procedures Mutual InformationI(X, Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)}"},{"title":"Heavy Tail Self-regression (HT-SR)","path":"/wiki/Loss_Landscape/Methmatic Tools/Heavy Tail Self-regression (HT-SR).html","content":"这是一篇关于 Traditional and Heavy-Tailed Self Regularization in Neural Network Models 的笔记. Setup 设 $W \\in \\mathbb{R}^{N \\times M}$ 是一个随机矩阵, $N$ 是样本数, $M$ 是权重的维度. Marchenko-Pastur (MP) theory Marchenko-Pastur (MP) 理论是一个用于描述随机矩阵的谱分布的理论.Marchenko-Pastur (MP) theory$W$ 的经验谱分布 $\\rho_{W}(\\lambda)$ 可以由以下公式给出:\\rho_{N}(\\lambda) \\xrightarrow[Q \\text{ fixed}]{N \\rightarrow \\infty} \\begin{cases} \\displaystyle \\frac{Q}{2 \\pi \\sigma_{mp}^{2}} \\frac{\\sqrt{(\\lambda^{+} - \\lambda) (\\lambda - \\lambda^{-})}}{\\lambda}, & \\text{if } \\lambda \\in [\\lambda^{-}, \\lambda^{+}] \\\\ 0, & \\text{otherwise}\\end{cases}其中, $\\lambda^{\\pm} = (1 \\pm \\frac{1}{\\sqrt{Q}})^{2}$, $\\sigma_{mp}^{2} = 1$, $Q = \\frac{N}{M} \\geq 1$.最大特征值 $\\lambda_{max}$ 的分布 $\\rho_{\\infty}(\\lambda_{max})$ 由 Tracy-Widom 定律控制. Heavy-Tailed extensions of MP theory MP 理论是基于高斯假设的, 但在实际应用中, 权重矩阵中的元素存在强相关性, 且不满足高斯假设, 反而更接近于重尾分布.最近的研究表明，对于某些重尾分布，MP理论存在新的普适类(Universality class)。我们假设 $W$ 的元素服从独立同分布的重尾分布, 且满足以下条件:Heavy-Tailed对于服从 $W_{ij} \\sim P(x)=\\frac{1}{x^{1+\\mu}},\\ \\mu \\gt 0$ 的权重矩阵 $W$, MP 理论存在以下三种新的普适类:弱重尾 ($4 \\lt \\mu$)中重尾 ($2 \\lt \\mu \\lt 4$)强重尾 ($0 \\lt \\mu \\lt 2$) 通过拟合幂律分布的指数 $\\alpha$, 可以判断出对应的μ值, 从而确定矩阵属于哪一个重尾普适类. Empirical results 小模型对于较老和较小的DNN模型 (如LeNet5和一个小型的MLP3模型), 其权重矩阵的ESD表现出弱自正则化特征，可以用MP理论的扰动变体”尖峰协方差模型”很好地建模. 在这种情况下, 少数特征值从随机矩阵本体中拉出, 导致MP软秩和稳定秩都降低. 这种弱自正则化类似于Tikhonov正则化, 因为存在一个”尺度”将”信号”与”噪声”清晰分离, 但又不同于显式的Tikhonov正则化, 因为它是DNN训练过程本身隐式导致的。 大模型对于现代的大型DNN模型 (包括AlexNet、Inception等), 其权重矩阵的ESD与基于高斯分布的标准MP模型有很大偏离, 它们反而似乎属于重尾随机矩阵模型的某个普适类, 即”重尾自正则化”. ESD呈现重尾特征，但有有限支撑. 在这种情况下，即使在理论上也没有一个”尺度”能清晰地将”信号”与”噪声”分离."},{"title":"Mode Connectivity (MC)","path":"/wiki/Loss_Landscape/Methmatic Tools/Mode Connectivity (MC).html","content":"Setup考虑一个样本数为 $n$ 的数据集 $S_{train} = \\left\\{ (x_{1}, y_{1}), \\cdots, (x_{n}, y_{n}) \\right\\}$. 我们的目标是去优化这样一个损失函数: Loss Function\\mathcal{L} (\\theta) = \\frac{1}{n} \\sum_{(x,y) \\in S_{train}} \\ell (f_{\\theta}(x), y) + \\lambda \\lVert \\theta \\rVert_{2}^{2} \\tag{1}$\\ell$ 在这里取 cross entropy loss. $\\lambda$ 是 weight decay, 用来控制 L2 正则项. 我们用 SGD 来优化模型, 迭代形式为: SGD\\theta \\leftarrow \\theta - \\eta abla g(\\theta), \\quad g(\\theta) = \\frac{1}{B} \\sum_{j=1}^B abla \\ell(f(x_j), y_j)\\tag{2}$\\eta$ 和 $B$ 分别是 learning rate, batch size 我们这里考察的是一个分类问题, 它的准确率为: acc训练集的准确率表示为:acc_{train} (\\theta) = \\frac{1}{n} \\sum_{(x,y) \\in S_\\text{train}} \\mathbb{1}(f_\\theta(x) = y)\\tag{3}测试集的准确率 $acc_{test}$ 同理. Mode connectivityMode Connectivity 是用来衡量两个参数配置 $\\theta$ 和 $\\theta’$ 之间的连接性.它通过寻找一条连接这两个配置的最低能量路径 $\\gamma_{\\phi}(t)$ 来描述它们之间的连通路径. 最低能量路径路径 $\\gamma(t)$ 的能量表示为:\\int \\mathcal{L} (\\gamma(t)) dt\\tag{4} 这条路径是通过 Bezier 曲线参数化得到的，其中 $t \\in[0, 1]$，并且路径上的能量（即损失函数 $L$ 的积分 $\\int \\mathcal{L}(\\gamma(t)) dt$）被最小化."}]